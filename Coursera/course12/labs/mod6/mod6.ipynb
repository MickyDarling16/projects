{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3773bdb5",
   "metadata": {},
   "source": [
    "# **Exercise 1: Start a Spark Standalone Cluster**\n",
    "In this exercise, you will initialize a Spark Standalone Cluster with a Master and one Worker.\n",
    "Next, you will start a PySpark shell that connects to the cluster and open the Spark Application\n",
    "Web UI to monitor it. We will be using the Theia terminal to run commands and docker-based\n",
    "containers to launch the Spark processes.\n",
    "\n",
    "**Task A**: Download [cars.csv](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/cars.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98f5772",
   "metadata": {},
   "source": [
    "### **Task B:** Initialize the Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a583235",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Download spark-master image if it does not exist and create the container\n",
    "docker run \\\n",
    "    --name spark-master \\\n",
    "    -h spark-master \\\n",
    "    -e ENABLE_INIT_DAEMON=false \\\n",
    "    -p 4040:4040 \\\n",
    "    -p 8080:8080 \\\n",
    "    -v `pwd`:/home/root \\\n",
    "    -d bde2020/spark-master:3.1.1-hadoop3.2\n",
    "\n",
    "## Download spark-worker image if it does not exist and create 2 worker containers\n",
    "docker run \\\n",
    "    --name spark-worker-1 \\\n",
    "    --link spark-master:spark-master \\\n",
    "    -e ENABLE_INIT_DAEMON=false \\\n",
    "    -p 8081:8081 \\\n",
    "    -v `pwd`:/home/root \\\n",
    "    -d bde2020/spark-worker:3.1.1-hadoop3.2 # The image (final result of docker build using the dockerfile) used to create the container.\n",
    "\n",
    "docker run \\\n",
    "    --name spark-worker-2 \\\n",
    "    --link spark-master:spark-master \\\n",
    "    -e ENABLE_INIT_DAEMON=false \\\n",
    "    -e SPARK_WORKER_WEBUI_PORT=8082 \\\n",
    "    -p 8082:8082 \\\n",
    "    -v `pwd`:/home/root \\\n",
    "    -d bde2020/spark-worker:3.1.1-hadoop3.2 \n",
    "# \"-e SPARK_WORKER_WEBUI_PORT=8082\" because in the dockerfile, only 8081 is open for all workers, this MAKES worker 2 open 8082 by force so it can use it else you won't be able to access the worker UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7561bc9a",
   "metadata": {},
   "source": [
    "### **Task C:** Connect a PySpark Shell to the Cluster, Open the UI and Create a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2238afa2",
   "metadata": {},
   "source": [
    "#### 1. Connect to Pyspark\n",
    "**NOTE:** This step is needed only if we were running the upcoming steps in an interactive terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82a7acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# docker exec (runs) → pyspark (starts) → pyspark (connects to) → spark-master service.\n",
    "docker exec \\\n",
    "    -it `docker ps | grep spark-master | awk '{print $1}'` \\\n",
    "    /spark/bin/pyspark \\\n",
    "    --master spark://spark-master:7077"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2a2deb",
   "metadata": {},
   "source": [
    "#### 2. Create a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5eced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1adadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('cars_csv_processor').master('spark://spark-master:7077').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ee52e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/home/root/cars.csv\", header=True, inferSchema=True) \\\n",
    "        .repartition(32) \\\n",
    "        .cache()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0442908",
   "metadata": {},
   "source": [
    "# **Exercise 2: Run an SQL Query and Debug in the Application UI**\n",
    "In this exercise, you will define a user-defined function (UDF) and run a query that results in an\n",
    "error. We will locate that error in the application UI and find the root cause. Finally, we will\n",
    "correct the error and re-run the query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cfec6c",
   "metadata": {},
   "source": [
    "### **Task A:** Run an SQL Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0763dbaa",
   "metadata": {},
   "source": [
    "#### 1. Define a UDF to show engine type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474fe5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95959da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(\"string\")\n",
    "def engine(cylinders):\n",
    "    time.sleep(0.2)  # Intentionally delay task\n",
    "    eng = {4: \"inline-four\", 6: \"V6\", 8: \"V8\"}\n",
    "    return eng.get(cylinders, \"other\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cd5763",
   "metadata": {},
   "source": [
    "#### 2. Add the UDF as a column in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9f8822",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"engine\", engine(\"cylinders\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d86e9b",
   "metadata": {},
   "source": [
    "##### 3. Group the DataFrame by “cylinders” and aggregate other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbd5a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = df.groupby(\"cylinders\")\n",
    "dfa = dfg.agg({\"mpg\": \"avg\", \"engine\": \"first\"})\n",
    "dfa.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtsciencep12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
