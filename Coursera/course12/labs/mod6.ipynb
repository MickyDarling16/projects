{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3773bdb5",
   "metadata": {},
   "source": [
    "# **Exercise 1: Start a Spark Standalone Cluster**\n",
    "In this exercise, you will initialize a Spark Standalone Cluster with a Master and one Worker.\n",
    "Next, you will start a PySpark shell that connects to the cluster and open the Spark Application\n",
    "Web UI to monitor it. We will be using the Theia terminal to run commands and docker-based\n",
    "containers to launch the Spark processes.\n",
    "\n",
    "**Task A**: Download [cars.csv](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/cars.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98f5772",
   "metadata": {},
   "source": [
    "### **Task B:** Initialize the Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a583235",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Download spark-master image if it does not exist and create the container\n",
    "docker run \\\n",
    "    --name spark-master \\\n",
    "    -h spark-master \\\n",
    "    -e ENABLE_INIT_DAEMON=false \\\n",
    "    -p 4040:4040 \\\n",
    "    -p 8080:8080 \\\n",
    "    -v `pwd`:/home/root \\\n",
    "    -d bde2020/spark-master:3.1.1-hadoop3.2\n",
    "\n",
    "## Download spark-worker image if it does not exist and create 2 worker containers\n",
    "docker run \\\n",
    "    --name spark-worker-1 \\\n",
    "    --link spark-master:spark-master \\\n",
    "    -e ENABLE_INIT_DAEMON=false \\\n",
    "    -p 8081:8081 \\\n",
    "    -v `pwd`:/home/root \\\n",
    "    -d bde2020/spark-worker:3.1.1-hadoop3.2 # The image (final result of docker build using the dockerfile) used to create the container.\n",
    "\n",
    "docker run \\\n",
    "    --name spark-worker-2 \\\n",
    "    --link spark-master:spark-master \\\n",
    "    -e ENABLE_INIT_DAEMON=false \\\n",
    "    -e SPARK_WORKER_WEBUI_PORT=8082 \\\n",
    "    -p 8082:8082 \\\n",
    "    -v `pwd`:/home/root \\\n",
    "    -d bde2020/spark-worker:3.1.1-hadoop3.2 \n",
    "# \"-e SPARK_WORKER_WEBUI_PORT=8082\" because in the dockerfile, only 8081 is open for all workers, this MAKES worker 2 open 8082 by force so it can use it else you won't be able to access the worker UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7561bc9a",
   "metadata": {},
   "source": [
    "### **Task C:** Connect a PySpark Shell to the Cluster, Open the UI and Create a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2238afa2",
   "metadata": {},
   "source": [
    "#### 1. Connect to Pyspark\n",
    "**NOTE:** This step is needed only if we were running the upcoming steps in an interactive terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82a7acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# docker exec (runs) → pyspark (starts) → pyspark (connects to) → spark-master service.\n",
    "docker exec \\\n",
    "    -it `docker ps | grep spark-master | awk '{print $1}'` \\\n",
    "    /spark/bin/pyspark \\\n",
    "    --master spark://spark-master:7077"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2a2deb",
   "metadata": {},
   "source": [
    "#### 2. Create a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb031c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1adadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/06 00:26:05 WARN Utils: Your hostname, MICKYXPS15 resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/12/06 00:26:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/06 00:26:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/06 00:26:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/12/06 00:26:17 ERROR TransportClient: Failed to send RPC RPC 5174377288581702508 to /172.17.160.1:4040: io.netty.channel.StacklessClosedChannelException\n",
      "io.netty.channel.StacklessClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source)\n",
      "25/12/06 00:26:17 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master 172.17.160.1:4040\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:108)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.io.IOException: Failed to send RPC RPC 5174377288581702508 to /172.17.160.1:4040: io.netty.channel.StacklessClosedChannelException\n",
      "\tat org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:395)\n",
      "\tat org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:372)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:999)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:860)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:877)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:863)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:968)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:856)\n",
      "\tat io.netty.handler.codec.MessageToMessageEncoder.write(MessageToMessageEncoder.java:113)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:881)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:863)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:968)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:856)\n",
      "\tat io.netty.handler.timeout.IdleStateHandler.write(IdleStateHandler.java:302)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:879)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:940)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1247)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t... 1 more\n",
      "Caused by: io.netty.channel.StacklessClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source)\n",
      "25/12/06 00:26:37 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from /172.17.160.1:4040 is closed\n",
      "25/12/06 00:26:37 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master 172.17.160.1:4040\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:108)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.io.IOException: Connection from /172.17.160.1:4040 closed\n",
      "\tat org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:147)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelInactive(TransportChannelHandler.java:117)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:305)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)\n",
      "\tat io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)\n",
      "\tat io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:277)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)\n",
      "\tat io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)\n",
      "\tat org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:225)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:305)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:301)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:813)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t... 1 more\n",
      "25/12/06 00:26:57 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from /172.17.160.1:4040 is closed\n",
      "25/12/06 00:26:57 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master 172.17.160.1:4040\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:108)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.io.IOException: Connection from /172.17.160.1:4040 closed\n",
      "\tat org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:147)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelInactive(TransportChannelHandler.java:117)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:305)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)\n",
      "\tat io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)\n",
      "\tat io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:277)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)\n",
      "\tat io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)\n",
      "\tat org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:225)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:305)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:301)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:813)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t... 1 more\n",
      "25/12/06 00:27:17 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\n",
      "25/12/06 00:27:17 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/06 00:27:18 WARN StandaloneAppClient$ClientEndpoint: Drop UnregisterApplication(null) because has not yet connected to master\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('cars_csv_processor').master('spark://172.17.160.1:7077').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e0f841",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ee52e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/home/root/cars.csv\", header=True, inferSchema=True) \\\n",
    "        .repartition(32) \\\n",
    "        .cache()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0442908",
   "metadata": {},
   "source": [
    "# **Exercise 2: Run an SQL Query and Debug in the Application UI**\n",
    "In this exercise, you will define a user-defined function (UDF) and run a query that results in an\n",
    "error. We will locate that error in the application UI and find the root cause. Finally, we will\n",
    "correct the error and re-run the query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cfec6c",
   "metadata": {},
   "source": [
    "### **Task A:** Run an SQL Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0763dbaa",
   "metadata": {},
   "source": [
    "#### 1. Define a UDF to show engine type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474fe5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95959da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(\"string\")\n",
    "def engine(cylinders):\n",
    "    time.sleep(0.2)  # Intentionally delay task\n",
    "    eng = {4: \"inline-four\", 6: \"V6\", 8: \"V8\"}\n",
    "    return eng.get(cylinders, \"other\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cd5763",
   "metadata": {},
   "source": [
    "#### 2. Add the UDF as a column in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9f8822",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"engine\", engine(\"cylinders\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d86e9b",
   "metadata": {},
   "source": [
    "##### 3. Group the DataFrame by “cylinders” and aggregate other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbd5a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = df.groupby(\"cylinders\")\n",
    "dfa = dfg.agg({\"mpg\": \"avg\", \"engine\": \"first\"})\n",
    "dfa.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtsciencep12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
